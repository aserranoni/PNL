\PassOptionsToPackage{dvipsnames}{xcolor}

\documentclass[12pt,twoside,a4paper]{article}
\usepackage{packages}
\title{MAP5747 Programação Não Linear: Alguns Exercícios}
\author{Ariel Serranoni}
\date{2º semestre de 2019}

\begin{document}

\maketitle
\tableofcontents
\section{Lista 1}
\begin{problema}
  Seja \(f\colon\Reals^n\to\Reals\) e sejam
  \(B\subseteq A\subseteq\Reals^n\). Se
 \(\inf_{x\in\Reals^n}f(x)=\alpha\in\Reals\), então
\begin{enumerate}[label=(\roman*)]
\item \(\inf_{x\in A}f(x)\leq\inf_{x\in B}f(x)\);
\item todo minimizador de \(f\) em \(A\) é um minimizador de \(f\)
  em \(B\).
\end{enumerate}
\end{problema}
\begin{proof}[Solução]\hfill
  \begin{enumerate}[label=(\roman*)]
  \item \[\inf_{x\in A} f(x)=
      \min\{\inf_{x\in B}f(x), \inf_{x\in A\setminus B}f(x)\}\leq
      \inf_{x\in B}f(x).\]
  \item Seja \(x\) tal que \(f(x)\leq f(y)\) para cada \(y\in A\). Como
    \(B\subseteq A\) temos que \(f(x)\leq f(y)\) para cada \(y\in B\).
    Logo,
    \(x\) minimiza \(f\) em \(B\).\qedhere
    \end{enumerate}
\end{proof}

\begin{problema}
Exercício 2 - Lista 1
\end{problema}
\begin{proof}[Solução]
  Considere a função \(f\colon\Reals\to\Reals\) dada por
  \(f(x)\coloneqq\exp(x)\). Considere \(\Omega=\Naturals\).
  Então cada ponto \(\bar{x}\in\Omega\) minimiza \(f\) localmente e,
  como \(f\) é injetora temos que \(f(x)\not=f(y)\)
  sempre que \(x\not= y\).\qedhere
\end{proof}

\begin{problema}
Exercício 3 - Lista 1 
\end{problema}
\begin{proof}[Solução]
 
Seja \(\{x_k\}_{k\in\Naturals}\subseteq \Omega\) uma sequência qualquer e
considere a sequência \(\{f(x_k)\}_{k\in\Naturals}\subseteq\Reals\). Como
\(\Omega\) é compacto temos que \(\{x_k\}_{k\in \Naturals}\) admite uma
subsequência convergindo para algum \(x\in\Omega\). Neste caso, segue que
\(\{f(x_k)\}_{k\in\Naturals}\) também admite uma subsequência convergindo
para \(f(x)\). Como \(x\in \Omega\) temos que
\(f(x)\in f(\Omega)\). Mostramos
assim que cada sequencia em \(f(\Omega)\) admite uma
subsequência convergindo
para um elemento do próprio \(f(\Omega)\), ou seja, \(f(\Omega)\) é
compacto.

Finalmente, vamos mostrar que \(\alpha\coloneqq\inf_{x\in\Omega}f(x)\in
f(\Omega\)$\backslash$) e
\(\beta\coloneqq\sup_{x\in\Omega}f(x)\in f(\Omega)\).
Como \(f(\Omega)\) é fechado temos que
\(f(\Omega)=\overline{f(\Omega)}\). Portanto é suficiente mostrar que 
\(\alpha,\beta\in\overline{f(\Omega)}\). Seja
\(\varepsilon\in\Reals_{++}\) e note que se
\(\alpha +\varepsilon\mathbb{B}\cap f(\Omega)=\varnothing\)
então \(\inf_{x\in\Omega}f(x)=\inf f(\Omega)\geq\alpha+\varepsilon\).
Isso implica que \(\inf f(\Omega) > \alpha\).
Contradição. [escrevemos analogamente pra \(\beta\)].
\end{proof}

\begin{problema}
Exercício 4 - Lista 1
\end{problema}
\begin{proof}[Solução]
  Considere a função
  \(f\colon\Reals\setminus\{0\}\to\Reals\setminus\{0\}\)
  dada por \(f(x)\coloneqq\frac{1}{x}\). Se consideramos
  \(\Omega=[-1,0)\), temos que \(f\) é contínua em \(\Omega\) e
  que \(\Omega\) é limitado, mas não fechado.
  Portanto não vale o Teorema de Bolzano-Weierstrass e \(f\) não possui
  minimizador, de fato \(f\) é ilimitada em \(\Omega\).
  Similarmente, se \(\Omega=[-1,0]\) temos que \(\Omega\) é compacto
  mas \(f\) não é contínua em \(\Omega\) e tb n vale o teorema.
\end{proof}

\begin{problema}
Exercício 5 - Lista 1   
\end{problema}
\begin{proof}[Solução]
  Como \(f\) é contínua, temos que o conjunto de nível
  dado no enunciado é fechado. Além disso, temos
  por hipótese que o conjunto é limitado. Assim, o
  resultado segue aplicando o exercicio 3.
\end{proof}

\begin{problema}
Exercício 6 - Lista 1  
\end{problema}
\begin{proof}[Solução]
  Seja \(x\in\Reals^n\) e considere o conjunto de nível
  \[N\coloneqq\{y\in\Reals^n\,\colon f(y)\leq f(x)\}.\]
Como \(f\) é contínua temos que \(N\) é fechado. Agora suponha que
\(N\) não é limitado,então existe uma sequencia \(\{y_n\}_{n\in\Naturals}\)
tal que \(\|y_n\|\rightarrow\infty\) mas \(f(y_n)\leq f(x)\) para
todo \(n\in\Naturals\), o que contradiz a hipótese de que
\(f\) é coerciva. Assim concluímos que \(N\) é compacto e
o resultado segue do exercicio 3.
\end{proof}

\begin{problema}
Exercício 7 - Lista 1
\end{problema}
\begin{proof}[Solução]\hfill
  \begin{enumerate}
\item Considere \(f(x)=\exp(x)\) e \(\Omega=\{0\}\).
\item Considere \(f(x)=-x^2\) e \(\Omega=\{0\}\).
\item Considere \(f(x)=x^3\) e \(\Omega=\Reals\).
\item Considere \(f(x)=x^3\) e \(\Omega=\Reals\).\qedhere
\end{enumerate}
\end{proof}


\section{Lista 1 - Old}


\begin{problema}\label{rosenmin}
Exercício 2 - 2.1 do NOCEDAL
\end{problema}
\begin{proof}[Solução]
  Iniciamos calculando uma forma polinomial para a função \(f\).
  Daí, obtemos que
  \begin{equation}\label{rosenfunc}
   f(x_1,x_2)=100x_1^4+x_1^2-2x_1+100x_2^2-200x_1^2x_2+1. 
  \end{equation}
   Além disso, vamos calcular o vetor gradiente e a matriz hessiana de \(f\):
\begin{equation}\label{gradrosen}
  \nabla f(x_1,x_2)=\begin{pmatrix}
    400x_1^3 + 2x_1-400x_1x_2 -2 \\
    200 x_2 - 200x_1^2
  \end{pmatrix}
  \text{ e } \nabla^2 f(x_1,x_2)=\begin{pmatrix}
    1200x_1^2+2-400x_2 & -400x_1 \\
    -400x_1 & 200
    \end{pmatrix}.
\end{equation}
Resolvendo o sistema \(\nabla f(x_1,x_2)=0\) nos dá a solução única
\(x\coloneqq(1,1)^\top\). Feito isso verificamos que
\[\nabla^2f(1,1)=\begin{pmatrix}
    802 & -400 \\
    -400 & 200 \end{pmatrix}\in\mathbb{S}^n_{++}.\]
Assim, concluímos que \(x\) é o único minimizador global de \(f\).
\end{proof}
\begin{problema}
Exercício 3  
\end{problema}
\begin{proof}[Solução]
Queremos resolver o seguinte problema não linear
\begin{maxi}
  {}{xy(x-y)}{}{}  
    \addConstraint{x+y}{=8}
    \addConstraint{x}{\geq 0}
    \addConstraint{y}{\geq 0}.
  \end{maxi}
Utilizando a primeira restrição para obter que \(y=8-x\) e substituindo na
função objetivo, obtemos o problema
\begin{maxi}
  {}{x(8-x)(x-())}{}{}  
  \addConstraint{x+y}{=8}
  \addConstraint{x}{\geq 0}
  \addConstraint{y}{\geq 0}.
\end{maxi}
\end{proof}

\begin{problema}
  Exercício 7
\end{problema}
\begin{proof}[Solução]
  Como \(A\) é positiva definida temos que \(f\) é convexa. Além disso
  sabemos que \(\nabla f(x)=Ax-b\). Daí, segue que
  \(\nabla f(x)=0\) se, e somente se \(Ax=b\). Dessa forma segue que
  o conjunto dos minimizadores de \(f\) é dado por
  \[\{x\in\mathbb{R}^n\,\colon Ax=b\}.\qedhere\]
\end{proof}

\begin{problema}
  Exercício 14***
\end{problema}
\begin{proof}[Solução]
  Primeiro calculamos algumas coisas, obtendo
  \begin{enumerate}
  \item \(\phi(\alpha)=f(\alpha
    d)=\frac{\alpha^4d_2^4}{2}+\alpha^2d_1^2-\frac{3}{2}\alpha^3d_1d_2^2.\)
  \item \(\phi^\prime(\alpha)=2\alpha^3d_2^4+ 2\alpha d_1^2 -
    \frac{9}{2}\alpha^2d_1d_2^2.\)
  \item \(\phi^{\prime\prime}=6\alpha^2d_2^4+2d_1^2-9\alpha d_1d_2.\)
  \end{enumerate}
 Daí vemos que \(\phi^\prime(0)=0\) e que \(\phi^{\prime\prime}(0)\geq 0\) e que
 essa última desigualdade vale estritamente sempre que \(d_1\not =0\). Além
 disso, note que \(f(0)=0\). Por fim, seja \(\varepsilon\in\Reals_{++}\) 
\end{proof}

\begin{problema}
  Exercício 15
\end{problema}
\begin{proof}[Solução]
  
\end{proof}

\begin{problema}
 Exercício 17 - 2.17 da ANA  
 \end{problema}
 \begin{proof}[Solução]
   Fazendo algumas continhas, obtemos facilmente que
   \(f^\prime(x)= 3x^2+2ax+b\) e
  ainda que \(f^{\prime\prime}(x)=6x+2a\). Para que \(0\) seja maximizador
  de
  \(f\), precisamos que \(f^\prime(0) =0\) e que \(f^{\prime\prime}(0)<0\).
  Analogamente para que \(1\) seja minimizador de \(f\) precisamos que
  \(f^\prime(1)=0\) e que \(f^{\prime\prime}(1)>0\). Resolvendo o sistema
  dado por estas equações obtemos a única soluçao \(b=0\) e \(a=-\frac{3}{2}\). 
 \end{proof}

\begin{problema}
Exercício 18
\end{problema}
\begin{proof}[Solução]
  Seja \(\overline{X}\coloneqq\{x\in X\,\colon f(x)=v^\ast\}\), sejam
  \(x_1,x_2\in\overline{X}\), e seja \(\lambda\in [0,1]\). Como \(f\)
  é convexa segue que \[f(\lambda x_1+ (1-\lambda)x_2)\leq\lambda
    f(x_1)+(1-\lambda)f(x_2)=v^\ast.\]
  Mas como \(v^\ast=\inf\{f(x)\,\colon x\in X\}\) também vale que
  \[f(\lambda x_1+ (1-\lambda)x_2)\geq v^\ast.\]
  Assim concluímos que \(f(\lambda x_1+ (1-\lambda)x_2)= v^\ast.\) Portanto,
  segue que \(\lambda x_1 + (1-\lambda)x_2\in\overline{X}\) e logo
  \(\overline{X}\) é convexo.
\end{proof}

\begin{problema}\label{phiconv}
 Exercício 19
\end{problema}
\begin{proof}[Solução]
  Sabemos que \(f\) é convexa se, e só se
  \(\nabla^2f(x)\in\mathbb{S}^n_+\) para cada \(x\in\Reals^n\). Observando
  que \(\phi^{\prime\prime}(\alpha)=d^\top\nabla^2f(x+\alpha d) d\),
  concluímos que \(\phi^{\prime\prime}(\alpha)\geq 0\) para cada
  \(\alpha\in\Reals\). Como este último fato acontece se e só se \(\phi\)
  é convexa, o resultado segue.
\end{proof}

\begin{problema}
Exercício 20 [CORREÇÃO NO ENUNCIADO: PODEMOS ASSUMIR QUE A MATRIZ \(A\) É
POSITIVA SEMIDEFINIDA]  
\end{problema}
\begin{proof}[Solução]
  Basta notar que \(\nabla^2 f(x)=A\). Daí temos que \(f\) é convexa e o
  resultado segue.
\end{proof}

\section{Lista 2 - Old}
\begin{problema}
 Exercício 1
\end{problema}
\begin{proof}[Solução]
  Por definição, a direção \(d\) é dada por \(d=-\nabla f(x)\). Segue
  \[d^\top\nabla f(x)=-\nabla f(x)^\top M\nabla f(x).\]
  Como \(M\) é positiva definida e \(\nabla f(x)\not =0\), obtemos que
  \(d^\top\nabla f(x)<0\) e portanto \(d\) é uma direção de descida para \(f\)
  a partir de \(x\).
\end{proof}
\begin{problema}
Exercício 3
\end{problema}
\begin{proof}[Solução]
  Primeiro, calculamos
  \[\lim_{t}\frac{\alpha_{t+1}}{\alpha_t}=\lim_{t}\frac{t+1}{t} =1.\]
  Finalmente aplicamos o Exercício anterior (2 L2-OLD) para concluir que
  \(\{\alpha_t\}_{t\in\Naturals}\) converge sublinearmente para \(0\).
\end{proof}
\begin{problema}
Exercício 5  
\end{problema}
\begin{proof}[Solução]
  Neste exercício faremos uso das contas feitas no
  Exercício \ref{rosenmin}.
  \begin{enumerate}
  \item Primeiro, veja que \(d=-\nabla f(0,0)=(2,0)^\top\). Neste caso
    segue que
    \begin{align*}
      \phi(\alpha)=f(0+\alpha d)&=f((2\alpha,0)^\top)\\&=
      100((-2\alpha)^2)^2+(1-2\alpha^2)\\&=
      100(16\alpha^4)+4\alpha^2-4\alpha + 1.
    \end{align*}
  \item Primeiro, vamos calcular a direção de Newton. Por definição, segue
    \begin{align*}
      d= (-\nabla^2f((0,0)^\top)^{-1}\nabla f((0,0)^\top)=\begin{pmatrix}
        -\frac{1}{2} & 0 \\ 0 & -\frac{1}{200}\end{pmatrix}\begin{pmatrix}
        -2 \\ 0 
      \end{pmatrix}=\begin{pmatrix}1 \\ 0\end{pmatrix}.
    \end{align*}
    Daí, segue que
    \[f((\alpha,0)^\top)=100((-\alpha)^2)^2+(1-\alpha)^2= 100\alpha^4+\alpha^2-2\alpha+1.\qedhere\]
  \end{enumerate}
\end{proof}

\begin{problema}\label{alphabuscaex}
Exercício 6  
\end{problema}
\begin{proof}[Solução]
  Primeiramente, notamos que \(\nabla f(x)=Ax + b\) e \(\nabla^2 f(x)=A\).
  Como \(A\in\mathbb{S}^n_+\) temos que \(f\) é convexa e portanto pelo
  Exercício \ref{phiconv} temos que
  \(\phi(\alpha)\coloneqq f(x+ \alpha d)\) é convexa para todo
  \(x,d\in\Reals^n\). Assim, podemos calcular o minimizador de phi
  da seguinte maneira:
  \begin{align*}
    &\phi^\prime(\alpha)=0\\&
    \iff\nabla f(x+\alpha d)^\top d=0 \\&
    \iff (A(x+\alpha d) + b)^\top d=0 \\&
    \iff ((Ax+b)^\top +\alpha (Ad)^\top)d=0 \\&
    \iff \nabla f(x)^\top +\alpha d^\top d=0 \\&
    \iff \alpha=-\frac{\nabla f(x)^\top d}{d^\top A d}.\qedhere
   \end{align*}
 \end{proof}
 \begin{problema}
 Exercício 8  
\end{problema}
\begin{proof}[Solução]
  Primeiro, calculamos \(\phi(\alpha)\coloneqq f(x+\alpha d)\), obtendo
  \[\phi(\alpha)=(\frac{\alpha}{2}-(-1+\alpha))^2+\frac{(1-\frac{\alpha}{2})^2}{2}.\]
  Além disso, calculando \(\nabla f(x)\) obtemos o resultado \((1 -2)^\top\).
  Daí, vemos que para que \(\alpha\) satisfaça a condição de Armijo é necessário
  que
  \begin{align*}
\phi(\alpha)=f(x+\alpha d)\leq f(x)+\frac{1}{8}\alpha
    \nabla f(x)^\top d =\frac{3}{2}-\frac{3\alpha}{16}
  \end{align*}
 Finalmente vamos aplicar o algorítmo. Vamos verificar se \(\alpha=1\) satisfaz
 a desigualdade acima. Calculando \(\phi(1)\) obtemos o valor \(\frac{19}{8}\).
 Por outro lado, temos que \(\frac{3}{2}-\frac{3}{16}=\frac{21}{16}\) e como
 esse número é estritamente maior que \(\phi(1)\) o algorítmo rejeita
 \(\alpha=1\). Depois disso, testamos \(\alpha=\frac{1}{2}\). Computamos
 \(\phi(\frac{1}{2})\) para obter o valor \(\frac{27}{32}\). Depois calculamos
 \(\frac{3}{2}-\frac{3}{16}\frac{1}{2}=\frac{45}{32}\). Daí o algorítmo termina aceitando \(\alpha=\frac{1}{2}\).
\end{proof}
\begin{problema}
   Exercício 10
\end{problema}
\begin{proof}[Solução]
 Primeiramente, note que \(\nabla f(x)=Ax-b\) e que \(x_{t+1}=x_t-\alpha\nabla
 f(x_t)\). Assim, calculando \(\nabla f(x_{t+1})\) em termos de \(\nabla
 f(x_t)\), obtemos que
 \begin{align*}
   \nabla f(x_{t+1})&= A(x_{t+1}) -b\\&
   = A(x_t-\alpha\nabla f(x_t))-b\\&
   = Ax -\alpha A\nabla f(x_t) -b \\&
   = \nabla f(x_t) - \alpha A\nabla f(x_t).
 \end{align*}
 Finalmente, segue que
 \begin{align*}
   \nabla f(x_{t+1})^\top\nabla f(x_t)&
                                        = (\nabla f(x_t) - \alpha A\nabla f(x_t))^\top\nabla f(x_t)\\&
   = \nabla f(x_t)^\top\nabla f(x_t) -\alpha (A\nabla f(x_t))^\top\nabla f(x_t) \\&=\nabla f(x_t)^\top\nabla f(x_t) - \frac{\nabla f(x_t)^\top\nabla f(x_t)\nabla f(x_t)^\top A\nabla f(x_t)}{\nabla f(x_t)^\top A\nabla f(x_t)}\\&= \nabla f(x_t)^\top\nabla f(x_t) - \nabla f(x_t)^\top\nabla f(x_t) \\&= 0.\qedhere
  \end{align*}
\end{proof}

\begin{problema}
Exercício 11  
\end{problema}
\begin{proof}[Solução]
 Como estamos usando busca linear \textbf{exata}, temos para cada
 \(k\in\Naturals\):
 \[\phi^\prime(\alpha_k)=\nabla f(x_k+\alpha_kd_k)^\top d_k=\nabla f(x_{k+1})^\top d_k=0.\]
 No contexto do método do gradiente, temos \(d_k=-\nabla f(x_k)\). Logo
 \[-\nabla f(x_{k+1})\nabla f(x_k)=0.\]
 Daí, o resultado segue.\qedhere
\end{proof}

\begin{problema}
Exercício 16    
\end{problema}
\begin{proof}[Solução]
Como estamos aplicando o método de Newton, teremos
\(d_1=-\frac{f^\prime(x_1)}{f^{\prime\prime}(x_1)}\). Como \(x_1\) satisfaz
 \(f^\prime(x_1)=0\) teremos \(d_1=0\).
\end{proof}
  
\section{Outros Exercícios}
\subsection{Ana}
\begin{problema}
 Exercício 7.1
\end{problema}
\begin{proof}[Solução]
  \begin{enumerate}[label=(\roman)]
  \item Primeiro Problema:
  \begin{mini}
    {}{x_1^2 + x_2^2+ x_3^2 -2x_1x_2}{}{}  
    \addConstraint{2x_1+x_2}{=4}
    \addConstraint{5x_1 -x_2}{=8}.
  \end{mini}
  Primeiro, notamos que as restrições do problema podem ser reescritas
    como \(Ax=b\) onde \[A=\begin{pmatrix}
        2 & 1 & 0 \\ 5 & 0 & -1
      \end{pmatrix} \text{ e } b=\begin{pmatrix}4 \\ 8\end{pmatrix}\]
    Fazendo algumas contas, obtemos que \(\Ker(A)=\text{Span}(\{(1,-2,5)\})\). Usando
    isso para construir a parametrização, notamos que o ponto \((1,2,-3)\) é
    viável no nosso problema, e daí todo ponto viável é da forma
    \((1,2,-3)+\alpha (1,-2,5)\). Ou seja, pontos viáveis são da forma
    \((1+\alpha, 2 -2\alpha, -3 + 5\alpha)\). Note que a região viável do
    problema tem dimensão 1 assim como \(\Ker(A)\). Usamos a forma geral da
    região viável para considerar o seguinte problema sem restrições
     \begin{mini}
    {}{(1+\alpha)^2 + (2-2\alpha)^2+ (-3+5\alpha)^2
      -2(1+\alpha)(2-2\alpha)=34\alpha^2-36\alpha +10}{}{}  
    \addConstraint{\alpha}{\in\Reals}.
    \end{mini}
    Finalmente, vamos escrever as condições de otimalidade para cada um dos
    problemas acima. Para \label{const1}, se \(x^\ast\) é ótimo local então
    existe \(\lambda\in\Reals^2\) tal que \[\nabla f(x^\ast)=\begin{pmatrix}
        2x_1^\ast - 2x_2 \\ 2x_2 - 2x_1 \\ 2x_3\end{pmatrix}=\underbrace{\begin{pmatrix}
      2 & 5 \\ 1 & 0 \\ -1 &
      0\end{pmatrix}}_{A^\top}\underbrace{\begin{pmatrix}\lambda_1 \\
      \lambda_2\end{pmatrix}}_{\lambda}\]
    e também \(\nabla^2f(x^\ast)=\begin{pmatrix}a\end{pmatrix}\)
 
  \end{enumerate}
\end{proof}
\begin{problema}
 Exercício 7.2
\end{problema}
\begin{proof}[Solução]
  \begin{enumerate}
    \item Notemos que \(\nabla f(x,y)=(y,x)^\top\) e que \(\begin{pmatrix} 0 & 1
        \\ 1 & 0 \end{pmatrix}\). Assim, nenhum ponto satisfaz as condições
      nescessárias de otimalidade.
     \item Adicionando a restrição \(x+y=0\) acabamos tendo que minimizar
       \(-x^2\) com \(x\in\Reals\), e então o problema não tem solução e nem
       valor ótimo finito
     \item  Trocando a restrição por \(x-y=0\) acabamos tendo que minimizar
       \(x^2\) com \(x\in\Reals\), obtendo a solução \(x=0\).
      \item As restrições podem mudar tudo num problema.
    \end{enumerate}
  \end{proof}
  \begin{problema}
   Exercício 7.3
 \end{problema}
 \begin{proof}[Solução]
Temos que resolver o problema
     \begin{mini*}
    {}{\frac{1}{2}\|x\|^2}{}{}  
    \addConstraint{x_1+2x_2+2x_3}{=4}.
    \end{mini*}
Sabendo que a função objetivo é convexa e fazendo algumas contas obtemos que
\(x^\ast=\lambda^\ast(1,2,2)\). Substituindo este valor na restrição do problema
segue que \(\lambda=\frac{4}{9}\) e daí já podemos concluir que a solução é \((\frac{4}{9},\frac{8}{9},\frac{8}{9})^\top\).
\end{proof}
\begin{problema}
 Exercício 7.4 
\end{problema}
\begin{proof}[Solução]
Primeiro, note que podemos equivalentemente minimizar a função \(g(x)\coloneqq
\frac{1}{2}\|x\|^2\) com as mesmas restrições. Também notamos que como posto de \(A\) é
\(m\) a matriz \(AA^\top\) é inversível. Como \(\bar{x}\) é ótimo para o
problema temos que \(\bar{x}\) satisfaz as condições nescessárias de
otimalidade. Isto é, sabemos que existe \(\lambda\) tal que \(\nabla
g(\bar{x})=\bar{x}=A^\top \lambda\). Multiplicando esta última igualdade por
\(A\) concluímos que \(A\bar{x}=AA^\top\lambda\) e então como \(A\bar{x}=b\)
segue que \(\lambda= (AA^\top)^{-1}b\). Substituindo o valor de \(\lambda\) nas
condições de otimalidade segue que \(\bar{x}=A^\top(AA^\top)^{-1}b\).  
\end{proof}
\begin{problema}
Exercício 7.5  
\end{problema}
\begin{proof}[Solução]
A afirmação é verdadeira pois as hipóteses do enunciado são condições
suficientes de otimalidade local.
\end{proof}
\begin{problema}
Exercício 7.6  
\end{problema}
\begin{proof}[Solução]
 Seja \(m\coloneqq\text{rank}(Q)\) e considere a função a função
 \(\varphi(\alpha)\coloneqq f(x_0+Z\alpha)\), onde \(Z\colon
 \Reals^{n-m}\to\text{Ker}(Q)\).
 Daí temos que \(\varphi^{\prime\prime}(\alpha)=Z^\top Q Z > 0\) e portanto
 precisamos encontrar \(\bar{\alpha}\) tal que
 \(\varphi^\prime(\bar{\alpha})=0\). Segue
\begin{align*}
  \varphi^\prime(\alpha)&=Z^\top\nabla f(x_0+Z\alpha)
  \\&= Z^\top(Q(x_0+Z\alpha)+ p)\\&=Z^\top Qx_0+Z^\top Q Z\alpha + Z^\top p
\end{align*}
E portanto \(\varphi^\prime(\alpha)=0\) se e só se
\begin{align*}
  \alpha= - (Z^\top Q Z)^{-1}  Z^\top(Q x_0 + p).
\end{align*}
Assim concluímos que a solução ótima do problema pode ser escrita como
\[x_0-Z (Z^\top Q Z)^{-1}  Z^\top(Q x_0 + p).\]
\end{proof}
\begin{problema}
 Exercício 7.7
\end{problema}
\begin{proof}[Solução]
  Para \(x\) fixado, a solução do problema de otimização
  \begin{mini}
  {}{\|\nabla f(x)-p\|}{}{}  
    \addConstraint{p}{\in\Ker(A)}.
  \end{mini}
 é a projeção ortogonal de \(\nabla f(x)\) em \(\Ker(A)\). A relação desse vetor
 com o primeiro problema é que esta é uma direção viável de descida!
\end{proof}
\begin{problema}
Exercício 7.8 
\end{problema}
\begin{proof}[Solução]
Considere o seguinte problema de otimização:
\begin{mini}
  {}{\frac{1}{2}\|x-y\|^2}{}{}  
  \addConstraint{\begin{pmatrix}
    A & 0 \\ 0 & C\end{pmatrix}\begin{pmatrix} x \\
    y \end{pmatrix}}{=\begin{pmatrix} b \\ d \end{pmatrix}}.
\end{mini}
Vamos escrever as condições de otimalidade para este problema. Primeiro, note
que \(\nabla f (x,y) = (x,-y)=(A^\top\lambda_1, C^\top\lambda_2)\).
\end{proof}
\begin{problema}
  Exercício 8.1
\end{problema}

\subsection{Nocedal}
\begin{problema}
  Exercício 2.6
\end{problema}
\begin{proof}[Solução]
Seja \(x\) um mínimo local isolado, então existe uma vizinhança \(V\) de \(x\)
tal que \(x\) é o único mínimo local de \(f\) em \(V\). Neste caso, temos que
\(f(v)>f(x)\) para cada \(x\not = v\in V\). Logo \(x\) é mínimo local estrito em \(V\).  
\end{proof}

NOTE QUE NÂO VALE A VOLTA!!!!!

\subsection{P1 do Ano Passado}
\begin{problema}
  Exercício 4
\end{problema}
\begin{proof}[Solução]
Como \(x_1\in L_1\) e \(x_2\in L_2\), podemos escrever \(L_1=x_1+\alpha d\) e
\(L_2=x_2+\alpha d\). Além disso vamos considerar \(\phi_1(\alpha)\coloneqq
f(x_1+\alpha d)\) e \(\phi_2(\alpha)\coloneqq f(x_2+\alpha d)\). Como \(x_1\) e
\(x_2\) são minimizadores, também temos que
\(\phi_1^\prime(0)=\phi_2^\prime(0)=0.\) Sabendo que
\(\phi_i^\prime(\alpha)=\nabla f(x_i+\alpha d)^\top d\) para \(i=1,2\) e que
\(\nabla f(x)=Ax+b\), segue
\begin{enumerate}
\item \((Ax_1+b)^\top d= x_1^\top Ad +b^\top d = 0\);
\item \((Ax_2+b)^\top d= x_2^\top Ad+ b^\top d = 0\).  
\end{enumerate}

Subtraindo a primeira equação da segunda o resultado segue.

\end{proof}

\end{document}
